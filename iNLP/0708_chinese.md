# 讨论会：非注意力盲视

2023年7月8日

![讨论大纲](./0708_Inattentional blindness.assets/image-20230708234004149.png)

- 情景感知/态势感知：识别，理解，预测

- 反馈设计：触发方式，反馈模态，模态类型

- 深化解决方案，聚焦一个点，达到更好的效果或带来更多新意

> *免责声明：我对HCI（人机交互）了解甚少。作为外行人，我给出的建议在相关领域缺乏坚实的理论基础。*

## 我对非注意力盲视(IB)与LLM关系的看法

非注意力盲视(IB)的概念创新且重要，这对于传统的算法来说难以判断。对于没有"常识"的方法，它似乎是一种NP-hard问题。

然而，我相信，作为人类智能的反映，LLM可以部分地为解决非注意力盲视决策问题做出贡献。

### 直观的工程思路

- 首先，测试一些非注意力盲视发生的案例，看看LLM是否可以判断问题。我们可能需要一个基础的演示问题集，除了驾驶外还包括其他情况。我相信在心理学领域肯定存在一些数据集，这需要进一步的文献综述。

- 不像其他的疏忽问题，非注意力盲视更加模糊和有趣，驾驶员通常不会承认他们在开车时走神。这使得很难得到正确的反馈。我们可能需要先找到一个好的度量，评估驾驶员的真实状态。作为一个工程学生，我建议使用一些详细的统计数据进行精确的量化。例如，我们有：

  - **眼动/注视区域**，如果驾驶员心不在焉，瞳孔可能会表现出不同的行为，眼动追踪方法也可以提供一些深入的想法。这可以被转化为文本模式并发送给LLM。
    - 数据来自车内摄像头
  - **心率**，当驾驶员非注意力盲视时，他的心率可能会保持在较低和放松的状态，这可能会降低他对外部环境的警觉性
    - 数据来自智能手表
  - **手的位置**，这里的第二项工作可能不仅仅是接电话。当驾驶员非注意力盲视时，他可能会做许多琐碎的动作。例如，挖耳朵，抓头，喝水...这些都是容易的工作，但会让他们分心。
    - 数据来自车内摄像头
  - **区域信息**，如果驾驶员在他熟悉的日常路线中，他总会倾向于感到更自信，从而进入非注意力盲视状态
  - ....其他容易被转化为文本的模态

## LLM的能力

当我说"LLM的主要功能是生成"，我的意思是，当前LLM的基本思想只是模仿人类的说话能力。它很强大，但永远不会是AGI（人工通用智能），这还离我们当前的时代相当遥远。

LLM的学习能力来自于文本数据，而我们目前没有视觉-GPT4（可能会在后期发布），我们不能轻易地将图像数据转化为文本数据。"手动转化"步骤会导致大量的数据丢失。基于规则的感知永远无法达到人类的视觉理解，但我们仍然可以使用当前的LLM来*生成一些有趣的输出*，用于交互任务。

也许我们需要思考：如果我们的视觉模型只能处理中远距离的数据，那么我们的LLM应该如何利用时间间隔？也许他可以生成一些有效和实用的输出，而不仅仅是对危险场景的警告。

就我对我们的项目的理解，我认为有价值的一点是发现道路上的“潜在/潜伏风险”。