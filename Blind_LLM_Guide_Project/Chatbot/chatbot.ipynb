{"cells":[{"cell_type":"markdown","metadata":{"id":"2dLdwEIy91QX"},"source":["# The Autonomous Vehicle Co-pilot\n","Here is a chatbot, empowered by GPT-3.5, running to help the vehicle driver. \n","\n","He is set as a co-pilot, analyzing the environment based on other multi-modality input, which has been processed as text so he can reason and identify potential risk. \n","\n","\n","## Setup\n","Notice: if you tried to run this program on your linux server, \n","and you are in China,\n","make sure that you opened the vpn. \n","Otherwise the api account may be forbiddened!"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7N8bvwlT98KX"},"outputs":[],"source":["import os\n","import openai\n","os.environ[\"http_proxy\"]=\"127.0.0.1:10080\"\n","os.environ[\"https_proxy\"]=\"127.0.0.1:10080\"\n","\n","def read_from_file(filepath):\n","    with open(filepath, 'r') as file:\n","        return file.read().strip()\n","\n","openai.api_key  = read_from_file('key/apikey.cn')\n","\n","TEMPERATURE = 0"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"OwMBRMRp9_ze"},"outputs":[],"source":["def get_completion(prompt, model=\"gpt-3.5-turbo-16k-0613\"):\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        messages=messages,\n","        temperature= TEMPERATURE, # this is the degree of randomness of the model's output\n","    )\n","    return response\n","    # return response.choices[0].message[\"content\"]\n","\n","def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\",temp = TEMPERATURE):\n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        messages=messages,\n","        temperature= temp, # this is the degree of randomness of the model's output\n","    )\n","    # print(str(response.choices[0].message[\"content\"]))\n","    return response.choices[0].message[\"content\"]"]},{"cell_type":"markdown","metadata":{},"source":["## test on the api"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"xQp3kZHr-JYL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Our current situation on the road is as follows:\n","\n","- We are driving on a city street with green plants and banyan trees lining the sides, providing shade.\n","- There are bicycle lanes on both sides of the road.\n","- On the left bicycle lane, there are three cyclists moving slowly.\n","- On the right bicycle lane, there are two cyclists riding at a faster pace.\n","- Next to the bicycle lanes, there is a spacious pedestrian walkway.\n","- On the left side of the pedestrian walkway, there are five pedestrians walking.\n","- On the right side of the pedestrian walkway, there are four pedestrians, two of whom are jogging.\n","- We are approaching an intersection.\n","- The traffic signal at the intersection is displaying a green light, indicating that we can proceed.\n","- In front of us, there is a blue sedan driving at a moderate speed.\n"]}],"source":["blind_guide = \"\"\" \n","We are currently driving on a city street filled with green plants, with bicycle lanes on both sides of the road. On the left bicycle lane, there are three cyclists moving slowly, while on the right side, there are two cyclists riding at a faster pace.\n","Next, there is a spacious pedestrian walkway, with five pedestrians walking on the left side and four pedestrians on the right side, two of whom are jogging. Each side of the street is lined with banyan trees, providing ample shade for pedestrians to cool off.\n","We are heading towards an intersection. The traffic signal at the intersection is displaying a green light, indicating that we can proceed. Each direction at the intersection has only one lane, clearly marked. In front of us, there is a blue sedan driving at a moderate speed.\"\"\"\n","chat_history = [\n","    { 'role':'system','content': blind_guide},\n","    {'role':'system', 'content':'You are a co-pilot AI assistant designed to help drivers understand their surroundings.'},\n","    {'role':'user', 'content':'What is our current situation on the road?'},\n","]\n","\n","test_response = get_completion_from_messages(chat_history);print(test_response)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["792"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["len(blind_guide)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{'b1c9c847-3bda4659': [1, 10, 12, 19],\n"," 'b1d10d08-c35503b8': [7, 10, 83, 90],\n"," 'b1d22ed6-f1cac061': [12,\n","  29,\n","  38,\n","  36,\n","  35,\n","  52,\n","  51,\n","  66,\n","  67,\n","  68,\n","  77,\n","  78,\n","  100,\n","  107,\n","  104,\n","  105,\n","  157,\n","  153,\n","  162,\n","  163,\n","  166,\n","  173,\n","  181,\n","  186,\n","  176,\n","  160,\n","  194,\n","  197,\n","  170,\n","  186,\n","  234,\n","  236,\n","  250,\n","  251,\n","  295,\n","  304,\n","  309,\n","  295,\n","  324,\n","  293,\n","  357,\n","  368,\n","  413,\n","  415],\n"," 'b1dac7f7-6b2e0382': [1, 8, 49],\n"," 'b1f4491b-33824f31': [4,\n","  36,\n","  43,\n","  44,\n","  77,\n","  119,\n","  120,\n","  169,\n","  166,\n","  170,\n","  167,\n","  185,\n","  209,\n","  235],\n"," 'b2ae0446-4b0bfda7': [],\n"," 'b2be7200-b6f7fe0a': [1, 2, 6, 13, 22, 26, 29, 28, 31, 34, 39, 66, 67],\n"," 'b2bee3e1-80c787bd': [2, 6, 12],\n"," 'b2d22b2f-8302eb61': [18, 44, 46, 47, 51, 55, 62, 63],\n"," 'b2d502aa-f0b28e3e': [12, 16, 29, 30, 33],\n"," 'b4542860-0b880bb4': [93, 240, 270, 265, 295, 314, 307],\n"," 'cd09a73f-5f6b9212': [1, 12],\n"," 'cd17ff29-f393274e': [1, 2, 3, 4, 25, 31, 37, 47, 49, 68],\n"," 'cd26264b-22001629': [2,\n","  8,\n","  19,\n","  47,\n","  24,\n","  26,\n","  12,\n","  13,\n","  57,\n","  9,\n","  43,\n","  18,\n","  46,\n","  45,\n","  18,\n","  43,\n","  45,\n","  24,\n","  48,\n","  96,\n","  97,\n","  107,\n","  109,\n","  127,\n","  151,\n","  153]}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["sys_prompt_mot = read_from_file(\"key/mot_sys.md\")\n","usr_prompt_mot = read_from_file(\"key/mot_usr.md\")\n","START_USR = \"Based on the following sequence of frames, provide a concise description of the road scene, capturing significant events or movements of people:\"\n"]},{"cell_type":"markdown","metadata":{"id":"YGfYQGtK-Mb9"},"source":["# Data Integration\n"," Firstly, you'll need a way to convert the input from the vehicle's sensors (like LIDAR, RADAR, cameras, etc.) into a format that the chatbot can understand. This might involve using object detection algorithms to identify objects and their positions, then converting this data into a text description of the scene.\n","\n","\n","## buggy chatbot here\n","I need to find a good programming way of coding here"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"SjeMn4Ae-N8t"},"outputs":[],"source":["# def chat_with_assistant(user_input, context):\n","#     # Append the user input to the context\n","#     context.append({'role':'user', 'content':f\"{user_input}\"})\n","    \n","#     # Get the assistant's response\n","#     response = get_completion_from_messages(context)\n","    \n","#     # Append the assistant's response to the context\n","#     context.append({'role':'assistant', 'content':f\"{response}\"})\n","    \n","#     return context\n","\n","\n","# # Chat with the assistant\n","# context = chat_with_assistant('Hello, assistant!', chat_history)\n","# print(context)\n","\n","# # Continue the conversation\n","# context = chat_with_assistant('Tell me a joke', context)\n","# print(context)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["ChatGPT, based on the incoming road scenario data, I need you to assess the situation and provide us with critical information. \n","## input\n","The road scenario data will be provided in {'context'}, it contains the continuous frame information of video from dashcam in the vehicle. The autonomous system provides you with information on road layout, people's locations, their distances and angles from the dashcam, the surfaces they are on, and the confidence level of each detection.\n","## output \n","Your analysis should provide:\n","- an understanding of the context complexity (low, medium, or high), \n","- the number of persons, cars, and bikes present in the scene, \n","- an array detailing the danger level for each person, if applicable.\n","The output should be markdown format\n","\n","This analysis should be grounded in the current context and make predictions for the short term future to assist in autonomous driving. The key goal here is to identify potential risks to enhance the safety and efficiency of our journey."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["\"You are a co-pilot AI designed to analyze road scenario data and provide critical information about the situation. Based on the video frames from the dashcam and the data provided by the autonomous system, you will assess the context complexity, count the number of persons, cars, and bikes in the scene, and evaluate the danger level for each person. Your goal is to identify potential risks to enhance the safety and efficiency of the journey. The output should be in markdown format."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["sys_prompt = read_from_file(\"key/sys_prompt.md\")\n","usr_prompt = read_from_file(\"key/usr_prompt.md\")\n","\n","def get_sys_msg(content:str): \n","    system_msg = {\"role\": \"system\", \"content\": content}\n","    return  system_msg\n","def get_usr_msg(content:str): \n","    msg = {\"role\": \"user\", \"content\": content}\n","    return msg\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["data = \"\"\"INFO of 0016:\n","road 0 is at middle_down\n","sidewalk 1 is at right_down\n","sidewalk 2 is at right_down\n","person 0 is at middle_down\n","The [distance,angle] from person 0 to our dashcam is: [very close,89.97960036997033]\n","person 1 is at left_down\n","The [distance,angle] from person 1 to our dashcam is: [very close,-89.98811258767391]\n","person 2 is at left_down\n","The [distance,angle] from person 2 to our dashcam is: [very close,-89.98749824583163]\n","person 3 is at middle_down\n","The [distance,angle] from person 3 to our dashcam is: [very close,89.96852785395733]\n","person 4 is at left_down\n","The [distance,angle] from person 4 to our dashcam is: [very close,-89.99328360812777]\n","person 5 is at left_down\n","The [distance,angle] from person 5 to our dashcam is: [very close,-89.99173842984247]\n","person 6 is at left_down\n","The [distance,angle] from person 6 to our dashcam is: [very close,-89.99199265188705]\n","person 7 is at left_down\n","The [distance,angle] from person 7 to our dashcam is: [very close,-89.99241327380344]\n","Person 0 is on the road 0, sidewalk 1, sidewalk 2,his/her bbox is [1151.6272   683.91156 1259.8259  1029.4976 ]\n","Person 1 is on the road 0, sidewalk 1, sidewalk 2,his/her bbox is [520.33466 722.2189  556.2974  830.36035]\n","Person 2 is on the road 0, sidewalk 1, sidewalk 2,his/her bbox is [544.58014 737.0274  573.4954  827.68024]\n","Person 3 is on the road 0,his/her bbox is [1110.2614  723.2576 1128.289   741.3161]\n","Person 4 is on the road 0,his/her bbox is [205.18512 686.6346  222.12915 713.3797 ]\n","Person 5 is on the road 0, sidewalk 1,his/her bbox is [343.09924 702.2559  363.39508 733.56   ]\n","Person 6 is on the road 0,his/her bbox is [324.6748  691.9793  343.29242 718.48505]\n","Person 7 is on the road 0,his/her bbox is [291.81046 690.33636 306.7418  715.5451 ]\n","number of Surface mask, Road&sidewalk, People 's mask, actural people: (9, 3, 9, 8)\"\"\""]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["messages = [\n","    get_sys_msg(sys_prompt),\n","    get_usr_msg(usr_prompt),\n","    {\"role\": \"user\", \"content\": f\"here's the current road scenario data: {data}\"},\n","]\n","# response = get_completion_from_messages(messages)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Great, now I have a working chatGPT api as below:{{{\n","messages = [\n","    get_sys_msg(sys_prompt),\n","    get_usr_msg(usr_prompt),\n","    {\"role\": \"user\", \"content\": f\"here's the current road scenario data: {frame_data}\"},\n","]\n","response = get_completion_from_messages(messages)\n","}}}\n","For raw data stored in folder raw_data/ , there are info of video as below:\n","- raw_data/Info_Video_0001.txt\n","- raw_data/Info_Video_0002.txt\n","...\n","### task: I want you provide a double for-loop for me, here is the pesudo code:\n","\n","for all the Info txt in raw_data folder{\n","    for each frame in the txt{\n","        read the frame info into variable 'frame_data'\n","        response = get_completion_from_messages\n","        append the response into 'LLM_data/Result_Video_xxxx', here the xxxx is current video id\n","    }\n","}\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def read_frames_from_raw_file(file_path):\n","    frames = []\n","    current_frame_data = ''\n","    reading_frame = False\n","\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            if line.startswith('INFO of'):\n","                if reading_frame:\n","                    frames.append(current_frame_data.strip())\n","                    current_frame_data = ''\n","                reading_frame = True\n","\n","            if reading_frame:\n","                current_frame_data += line\n","\n","    # Adding the last frame if any\n","    if current_frame_data:\n","        frames.append(current_frame_data.strip())\n","\n","    return frames\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# import os\n","# import re\n","# from pathlib import Path\n","\n","# raw_data_folder = \"raw_data/\"\n","\n","# output_folder = \"LLM_data/\"\n","\n","# if not os.path.exists(output_folder):\n","#     os.makedirs(output_folder)\n","\n","# for filename in os.listdir(raw_data_folder):\n","#     if filename.startswith(\"Info_Video_\") and filename.endswith(\".txt\"):\n","#         # the id may be 'bd99', '0001', 'eb61'\n","#         video_name = filename[-8:-4]\n","#         output_path = Path(f\"LLM_data/Result_Video_{video_name}.txt\")\n","        \n","#         frames = read_frames_from_raw_file(os.path.join(raw_data_folder, filename))\n","#         start_frame = 0\n","#         if output_path.exists():\n","#             print(f\"Already handled {output_path}\")\n","#             # Read the existing frames that have been processed\n","#             # with open(output_path, 'r') as result_file:\n","#             #     processed_frames = re.findall(r'### Frame (\\d{4}) Analysis', result_file.read())\n","#             #     start_frame = len(processed_frames) # Start from the next unprocessed frame\n","#             #     frames = frames[start_frame:] # Excl\n","#             #     if frames is None:\n","#             #         continue\n","#             continue\n","#         with open(os.path.join(raw_data_folder, filename), 'r') as raw_file:\n","#             with open(os.path.join(output_folder, f\"Result_Video_{video_name}.txt\"), 'a') as result_file:\n","#                 print(f\"Analyzing video {video_name} starting from frame {start_frame}\")\n","\n","#                 for frame_data in frames:\n","#                     messages = [\n","#                         get_sys_msg(sys_prompt), \n","#                         get_usr_msg(usr_prompt), \n","#                         {\"role\": \"user\", \"content\": f\"here's the current road scenario data: {frame_data}\"},\n","#                     ]\n","#                     response = get_completion_from_messages(messages)\n","                    \n","#                     result_file.write(response + '\\n')\n","                    \n","#         print(f\"Analysis completed for video {video_name}\")\n","\n","# print(\"All videos processed!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2939\n"]}],"source":["import re\n","\n","input_file = \"./raw_data/person_bbox_nlp/mot_video_0194.txt\"\n","\n","def downsample_read_frame(file_path, step=2, segment_size=25):\n","    \"\"\"\n","    Temporally downsample the data based on frame number and segment the data into chunks.\n","\n","    Args:\n","    - file_path (str): Path to the file containing the data.\n","    - step (int): The step size for downsampling. Default is 2 (every second frame).\n","    - segment_size (int): The number of lines in each segment. Default is 200.\n","\n","    Returns:\n","    - list of str: The segmented data, each segment is a list of `segment_size` lines of downsampled data.\n","    \"\"\"\n","    with open(file_path, 'r') as f:\n","        frames = f.readlines()\n","\n","    downsampled_data = []\n","    for frame in frames:\n","        if frame.startswith(\"At frame\"):\n","            frame_number = int(re.search(r\"At frame (\\d+)\", frame).group(1))\n","            if frame_number % step == 0:\n","                downsampled_data.append(frame)\n","\n","    # Segment the downsampled data into chunks of size segment_size\n","    segments = [\"\".join(downsampled_data[i:i + segment_size]) for i in range(0, len(downsampled_data), segment_size)]\n","    return segments\n","\n","# Test the function\n","segments = downsample_read_frame(input_file)\n","\n","print(len(segments[0]))  # Displaying the first segment for brevity\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["'At frame 0, person 1 stands at coordinates [-21.95, 662.12] with a bbox of width 112.44 and height 309.52,conf is 0.84\\nAt frame 0, person 2 stands at coordinates [1098.65, 677.22] with a bbox of width 37.30 and height 86.15,conf is 0.77\\nAt frame 0, person 3 stands at coordinates [848.19, 709.19] with a bbox of width 29.31 and height 63.95,conf is 0.67\\nAt frame 0, person 4 stands at coordinates [180.30, 675.00] with a bbox of width 32.42 and height 85.26,conf is 0.65\\nAt frame 0, person 5 stands at coordinates [1078.22, 684.77] with a bbox of width 30.20 and height 70.16,conf is 0.61\\nAt frame 2, person 2 stands at coordinates [1106.75, 664.70] with a bbox of width 36.57 and height 84.28,conf is 0.68\\nAt frame 2, person 3 stands at coordinates [850.14, 701.16] with a bbox of width 29.51 and height 64.44,conf is 0.44\\nAt frame 2, person 4 stands at coordinates [166.48, 667.77] with a bbox of width 27.44 and height 72.28,conf is 0.63\\nAt frame 2, person 5 stands at coordinates [1085.93, 675.70] with a bbox of width 26.36 and height 60.99,conf is 0.49\\nAt frame 2, person 7 stands at coordinates [646.13, 701.64] with a bbox of width 18.66 and height 48.85,conf is 0.62\\nAt frame 4, person 2 stands at coordinates [1114.95, 649.44] with a bbox of width 38.75 and height 89.25,conf is 0.72\\nAt frame 4, person 3 stands at coordinates [852.74, 687.05] with a bbox of width 30.05 and height 65.84,conf is 0.49\\nAt frame 4, person 4 stands at coordinates [147.02, 655.88] with a bbox of width 30.98 and height 81.96,conf is 0.39\\nAt frame 4, person 5 stands at coordinates [1093.10, 661.37] with a bbox of width 29.14 and height 67.49,conf is 0.55\\nAt frame 4, person 7 stands at coordinates [645.51, 689.42] with a bbox of width 18.53 and height 48.39,conf is 0.58\\nAt frame 6, person 2 stands at coordinates [1124.36, 632.30] with a bbox of width 42.32 and height 97.89,conf is 0.78\\nAt frame 6, person 3 stands at coordinates [857.16, 672.14] with a bbox of width 31.02 and height 68.01,conf is 0.76\\nAt frame 6, person 4 stands at coordinates [127.10, 643.83] with a bbox of width 39.10 and height 103.33,conf is 0.72\\nAt frame 6, person 5 stands at coordinates [1101.62, 645.90] with a bbox of width 33.18 and height 77.11,conf is 0.61\\nAt frame 6, person 7 stands at coordinates [645.18, 675.45] with a bbox of width 19.04 and height 49.42,conf is 0.71\\nAt frame 8, person 2 stands at coordinates [1136.89, 622.68] with a bbox of width 40.85 and height 94.72,conf is 0.73\\nAt frame 8, person 3 stands at coordinates [862.80, 662.16] with a bbox of width 32.45 and height 71.14,conf is 0.73\\nAt frame 8, person 4 stands at coordinates [106.92, 634.11] with a bbox of width 40.74 and height 107.16,conf is 0.58\\nAt frame 8, person 5 stands at coordinates [1112.28, 634.58] with a bbox of width 31.98 and height 74.34,conf is 0.70\\nAt frame 8, person 7 stands at coordinates [645.41, 668.53] with a bbox of width 19.00 and height 49.15,conf is 0.69\\n'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["segments[0]"]},{"cell_type":"markdown","metadata":{},"source":["c"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["sys_prompt_mot = read_from_file(\"key/mot_sys.md\")\n","usr_prompt_mot = read_from_file(\"key/mot_usr.md\")\n","START_USR = \"Based on the following sequence of frames, provide a concise description of the road scene, capturing significant events or movements of people:\"\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'sys_prompt_mot' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\#Courses Folder\\##大三\\SU23\\Summer_Research_2023\\Blind_LLM_Guide_Project\\Chatbot\\chatbot.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%23Courses%20Folder/%23%23%E5%A4%A7%E4%B8%89/SU23/Summer_Research_2023/Blind_LLM_Guide_Project/Chatbot/chatbot.ipynb#X33sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAll videos processed!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%23Courses%20Folder/%23%23%E5%A4%A7%E4%B8%89/SU23/Summer_Research_2023/Blind_LLM_Guide_Project/Chatbot/chatbot.ipynb#X33sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Usage example:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%23Courses%20Folder/%23%23%E5%A4%A7%E4%B8%89/SU23/Summer_Research_2023/Blind_LLM_Guide_Project/Chatbot/chatbot.ipynb#X33sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m analyze_videos_with_llm(sys_prompt \u001b[39m=\u001b[39m sys_prompt_mot, usr_prompt \u001b[39m=\u001b[39m usr_prompt_mot)\n","\u001b[1;31mNameError\u001b[0m: name 'sys_prompt_mot' is not defined"]}],"source":["import os\n","from pathlib import Path\n","\n","def analyze_videos_with_llm(raw_data_folder=\"raw_data/person_bbox_nlp/\", output_folder=\"LLM_data/0829_MOT/\", \n","                            sys_prompt=\"System message\", usr_prompt=\"User prompt\"):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","    \n","    for filename in os.listdir(raw_data_folder):\n","        if filename.startswith(\"mot\") and filename.endswith(\".txt\"):\n","            video_name = os.path.basename(filename).split('.')[0]\n","            # print(video_name)\n","            input_path = Path(f\"{raw_data_folder}/{filename}\")\n","            output_path = Path(f\"{output_folder}/llm_{video_name}.txt\")\n","            # print(os.path.join(raw_data_folder, filename))\n","            if output_path.exists():\n","                print(f\"Already handled {output_path}\")\n","                continue\n","            frames = downsample_read_frame(input_path)\n","\n","            with open(os.path.join(output_folder, f\"llm_{video_name}.txt\"), 'w') as result_file:\n","                for i,frame_data in enumerate(frames):\n","                    print(f\"Analyzing video {video_name} starting from batch {i}\")\n","\n","                    print(frame_data[:30])\n","                    messages = [\n","                        get_sys_msg(sys_prompt),\n","                        get_usr_msg(usr_prompt), # put in the end can enhance its ability\n","                        {\"role\": \"user\", \"content\": f\"{START_USR} {frame_data}\"},\n","                    ]\n","                    response = get_completion_from_messages(messages)\n","                    \n","                    result_file.write(f'<-batch_{i}->\\n' + response + '\\n' )\n","                    \n","            print(f\"Analysis completed for video {video_name}\")\n","\n","    print(\"All videos processed!\")\n","\n","# Usage example:\n","analyze_videos_with_llm(sys_prompt = sys_prompt_mot, usr_prompt = usr_prompt_mot)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_information(input_text):\n","    # Find the position of \"Therefore\" in the text\n","    index = input_text.find(\"Therefore\")\n","    \n","    # If \"Therefore\" is found, extract the text after it\n","    if index != -1:\n","        information_after_therefore = input_text[index:]\n","    else:\n","        information_after_therefore = \"Keyword 'Therefore' not found in the text.\"\n","\n","    return information_after_therefore\n","\n","def write_to_file(output_text, file_name='output.txt'):\n","    with open(file_name, 'w') as file:\n","        file.write(output_text)\n","\n","if __name__ == \"__main__\":\n","    # Read input text from a file\n","    with open('input.txt', 'r') as file:\n","        input_text = file.read()\n","\n","    # Extract the information after \"Therefore\"\n","    output_text = extract_information(input_text)\n","\n","    # Write the output to another text file\n","    write_to_file(output_text)\n","\n","    print(f\"Information after 'Therefore' has been written to {output_file_name}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["2. **Contextual Understanding:** Your chatbot should be capable of maintaining and updating a model of the car's surroundings based on this information. For example, it should keep track of the positions of other vehicles, pedestrians, and road features, updating these as new data comes in.\n","\n","3. **Real-Time Interaction:** The chatbot should be capable of generating responses in real-time. Delays could be dangerous in a driving context.\n","\n","4. **Proactive Alerting:** Your co-pilot chatbot should not just react to the driver's queries but should also proactively provide alerts and advice. For example, if it detects a pedestrian stepping onto the road, it should immediately alert the driver.\n","\n","5. **Simulated Training:** Consider using simulated environments to train and test your chatbot before deploying it in a real vehicle. This can help you ensure that the chatbot behaves correctly in a wide range of situations.\n","\n","6. **Safety Precautions:** Since the bot is serving in a co-pilot role, it should prioritize safety and assume the driver may be unaware of potential dangers. It should also be prepared to handle ambiguous situations by advising caution.\n","\n","7. **User Customization:** Different drivers may have different preferences for how much information they want to receive, what kind of language the co-pilot uses, etc. Consider allowing users to customize the chatbot's behavior to some extent.\n","\n","8. **Integration with Vehicle's Systems:** If possible, the chatbot could also be integrated with the vehicle's systems to provide even more functionality. For example, it could adjust the vehicle's speed or even take control in an emergency. However, this would require a high level of reliability and many legal and ethical considerations."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["maybe this will help "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["video_info_str = \"\"\"\n","Video file: video_0001.mp4\n","Resolution: 1920x1080\n","Number of frames: 600\n","Frames per second: 29.97002997002997\n","Duration (seconds): 20.02\n","\n","Video file: video_0310.mp4\n","Resolution: 1920x1080\n","Number of frames: 150\n","Frames per second: 30.0\n","Duration (seconds): 5.0\n","\n","Video file: video_0194.mp4\n","Resolution: 1920x1080\n","Number of frames: 540\n","Frames per second: 30.0\n","Duration (seconds): 18.0\n","\n","Video file: video_0343.mp4\n","Resolution: 1920x1080\n","Number of frames: 720\n","Frames per second: 30.0\n","Duration (seconds): 24.0\n","\n","Video file: video_0003.mp4\n","Resolution: 1920x1080\n","Number of frames: 210\n","Frames per second: 29.97002997002997\n","Duration (seconds): 7.007000000000001\n","\n","Video file: video_0333.mp4\n","Resolution: 1920x1080\n","Number of frames: 210\n","Frames per second: 30.0\n","Duration (seconds): 7.0\n","\n","Video file: video_0313.mp4\n","Resolution: 1920x1080\n","Number of frames: 600\n","Frames per second: 30.0\n","Duration (seconds): 20.0\n","\n","Video file: video_0055.mp4\n","Resolution: 1920x1080\n","Number of frames: 210\n","Frames per second: 29.97002997002997\n","Duration (seconds): 7.007000000000001\n","\n","Video file: video_0056.mp4\n","Resolution: 1920x1080\n","Number of frames: 270\n","Frames per second: 29.97002997002997\n","Duration (seconds): 9.009\n","\n","Video file: video_0057.mp4\n","Resolution: 1920x1080\n","Number of frames: 240\n","Frames per second: 29.97002997002997\n","Duration (seconds): 8.008000000000001\n","\n","\n","\"\"\"\n","\n","def parse_video_info(video_info_str):\n","    # Split the string into individual video blocks\n","    video_blocks = video_info_str.strip().split(\"\\n\\n\")\n","    \n","    video_info_dict = {}\n","    \n","    for block in video_blocks:\n","        lines = block.split(\"\\n\")\n","        \n","        # Extract video file name\n","        video_file = lines[0].split(\": \")[1]\n","        \n","        # Extract resolution\n","        resolution = lines[1].split(\": \")[1]\n","        \n","        # Extract number of frames\n","        num_frames = int(lines[2].split(\": \")[1])\n","        \n","        # Extract frames per second\n","        fps = float(lines[3].split(\": \")[1])\n","        \n","        # Extract duration\n","        duration = float(lines[4].split(\": \")[1])\n","        \n","        # Populate the dictionary\n","        video_info_dict[video_file] = {\n","            \"resolution\": resolution,\n","            \"num_frames\": num_frames,\n","            \"fps\": fps,\n","            \"duration\": duration\n","        }\n","    \n","    return video_info_dict\n","\n","video_data = parse_video_info(video_info_str)\n","print(video_data)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPUB/l//BFK5z5KaeV64uDe","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
