# Driving Agent: LLM helps the risk warning on HUD in human-machine co-driving

## Background

### system design

In this module, we employ a cascade of cutting-edge vision processing algorithms to establish an Integrated Visual Analysis framework for LLM. The architectural backbone of this module is a multi-object tracking system powered by ByteTrack[cite: ] and YOLOv8. To reach an optimal balance for computational efficiency and llm logical processing applicability, we employ a Hybrid Frame Analysis strategy. Specifically, the backbone operates at full frame rates(30Hz), while other models are  executed at a reduced rate of 2Hz. 

% this is supported by paper todo    

To manage the massive data generated by these vision models, we have designed a streamlined  pipeline that distill this complex visual information into actionable text-based insights. For instance, we utilize Grounding DINO and SAM to obtain the relationship between sidewalks and roads, while DPT is deployed for depth estimation in the current road scene. Consequently, we extract the following attributes for each pedestrian: 

% I want this to be a box or items list

- The globally unique id throughout the video.
- The bounding box trajectory.
- The type of surface upon which the pedestrian is located.
- The overlap between multiple surfaces
- The spatial orientation, including distance and angle, relative to the ego vehicle.

This extracted data is then compressed and combined into text files, which are fed into GPT-3.5 Turbo 16K-0613. The LLM performs:

1. Pedestrian intention analysis to predict actions such as potential street crossings and their relative direction to the ego vehicle.

2. A comprehensive risk assessment, taking into account factors such as complexity and distance, to highlight areas of immediate concern within the current frame range.

   % to check : group complexity and 认知负荷

3. Binary classification of pedestrian behaviors to generate interval-based risk warnings, which are displayed on ARHUD during human-machine co-driving experiments.

Specifically, the image caption model constructs an image processing module to comprehend traffic semantics. For continuous sequences of traffic scene images, the text generation module generates coherent scene descriptions. Leveraging extensive human knowledge, the large-scale pre-trained language model employs potent language reasoning and semantic understanding techniques to predict intentions within the context descriptions. Within this cascading framework, the model acquires the ability to comprehend complex scenes and engage in intention reasoning.




### models

#### Object detection: Grounding DINO

- [2022SOTA]
- Transformer based knowledge distillation, SOTA zero-shot detection. 
- can generate bbox of the detected object using monocular image
- Can be prompted based on nature language
- The gDINO can successfully recoginze the road & sidewalk using contrast prompting
- Pedestrian detection is seperate, optimizing the accuracy
- This part can be replaced by yolo7, meeting equivalent performance while much faster

#### Segmentation: Segment Anything(SAM)

- [2023SOTA]
- Transformer based unlabeled segmentation, SOTA zero-shot segmentation
- can generate pixel level binary mask of the image
- Can be prompted by both point and box, including positive and negative prompt
- Combining with gDINO, then we realize a SOTA semantic segmentation model 

#### Depth estimation: Dense Prediction Transformer(DPT)

- [2021SOTA]
- Transformer based monocular depth estimation
- can generate image-size numpy array indicating the depth value of each pixel
- Based on it, we obtained the distance from dashcam to object

#### LLM: gpt3.5_turbo_16k

- long memory, can analyze the road scene with appropriate prompting
- relative cheap and fast model , 3500 tpm better fits for 



## Realization procedure

1. **Video Input and Preprocessing:** 

   - Use OpenCV to load video.
   - Extract frames from the video, 3Hz

2. **Frame-level Analysis:** 

   - For each frame, detect the regions of interest (pedestrians, road, sidewalk) using gDINO

   - Use bbox to prompt SAM, getting binary mask of detected object

   - Use DPT to obtain the depth information of each frame

   - Rule-based analyze the location and position relationship between pedestrians and surface under them. 

     

3. **Temporal Analysis:**

   - With the detected entities from each frame in obj_dict, we first align the detected object in each frame using `scipy's distance_matrix & linear_sum_assignment`
     - The closest pedestrian in sequential image is mapped with same id in the dictionary
   - Determine the movement speed of the pedestrians.
     - Achieve this by comparing the pixel location of each pedestrian in the current frame to their location in the previous frame.
   - Depending on the direction and speed of the pedestrian, determine the likely behavior.
     - For example, if the pedestrian is on the sidewalk and moving slowly, they may be walking. If they are on the road and moving quickly, they may be running or crossing the road.

4. **Load into text file**
   Record the behavior of all the pedestrian for each frame.

  ```markdown
- 6 area of the object
- location on which surface
- whether does two surface overlap each other (and percentage)
- distance and angle between object and dashcam
- number of detected object
 
  ```

5. **LLM Analysis**

- complexity of the road

- where the potential danger might be on the image

- who is the most dangerous pedestrian